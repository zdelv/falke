# falke

**NOTE**: This is generated by Claude. I'm not a Neovim plugin developer and I'm
not very interested in diving into the API.

A Neovim plugin for interacting with LLMs through OpenAI-compatible APIs. Edit
code with AI assistance directly in your editor using visual selections and
inline prompts.

Only works with `/v1/chat/completions` APIs.

## Requirements

- Neovim >= 0.8.0
- `curl` command-line tool

## Installation

I've only tested lazy.nvim. YMMV with the other package managers.

### Using [lazy.nvim](https://github.com/folke/lazy.nvim)

```lua
{
  'zdelv/falke',
  dependencies = {
    'j-hui/fidget.nvim'
  },
  config = function()
    require('falke').setup({
      endpoint = 'https://api.openai.com',  -- Your OpenAI-compatible endpoint
      api_key = 'your-api-key-here',        -- Your API key
    })
  end
}
```

## Configuration

### Option 1: Lua Setup (Recommended)

Configure the plugin in your `init.lua`:

```lua
require('falke').setup({
  endpoint = 'https://api.openai.com',  -- Required: Your API endpoint
  api_key = 'sk-...',                   -- Required: Your API key
  timeout = 30000,                      -- Optional: Request timeout in ms (default: 30000)
  model = 'gpt-4',                      -- Optional: Default model (auto-selected if not set)
  stream = true,                        -- Optional: Enable streaming responses (default: false)
})
```

### Option 2: Environment Variables

Set these environment variables before starting Neovim:

```bash
export LLM_ENDPOINT="https://api.openai.com"
export LLM_API_KEY="sk-..."
```

### Option 3: Both (Config takes precedence)

You can use both methods. Values in `setup()` will override environment variables.

## Usage

### Setting Up Keymaps

The plugin provides functions but no default keymaps. Add your own in `init.lua`:

```lua
-- Prompt visual selection with LLM
vim.keymap.set('v', '<leader>lp', function()
  require('falke').prompt_selection()
end, { desc = 'LLM: Prompt selection' })

-- Prompt full file with LLM
vim.keymap.set('n', '<leader>lf', function()
  require('falke').prompt_file()
end, { desc = 'LLM: Prompt full file' })

-- List available models
vim.keymap.set('n', '<leader>lm', function()
  require('falke').list_models()
end, { desc = 'LLM: List models' })

-- Get current model
vim.keymap.set('n', '<leader>lg', function()
  require('falke').get_current_model()
end, { desc = 'LLM: Get current model' })
```

### Commands

The plugin provides the following commands:

- `:LlmSetModel <model>` - Set the current model (e.g., `:LlmSetModel gpt-4`)
- `:LlmListModels` - Open a floating window with available models
- `:LlmGetModel` - Display the currently selected model
- `:LlmRefreshModels` - Refresh the cached model list
- `:LlmPromptFile` - Prompt with entire file (edit or generate from scratch)

### Streaming

The plugin supports streaming responses, which allows you to see the LLM's output
appear in real-time as it's generated. This provides a more interactive experience,
especially for longer responses.

To enable streaming, set `stream = true` in your setup:

```lua
require('falke').setup({
  endpoint = 'https://api.openai.com',
  api_key = 'sk-...',
  stream = true,  -- Enable streaming
})
```

When streaming is enabled:
- The loading indicator disappears when the first chunk arrives
- Code appears in the buffer incrementally as the LLM generates it
- You can watch the code being written in real-time
- Works with both visual selection and full file prompting

**Note**: Streaming uses Server-Sent Events (SSE) and requires an OpenAI-compatible
API that supports streaming with the `/v1/chat/completions` endpoint.

## License

MIT
